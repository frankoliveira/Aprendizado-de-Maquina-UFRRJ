{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"NeuralNetwork.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ntKT49rPafFh"},"source":["## Rede Neural Artificial (Multi Layer Perceptron)"],"id":"ntKT49rPafFh"},{"cell_type":"markdown","metadata":{"id":"hOG7QdOYbqdq"},"source":["Este trabalho consiste na implementação de uma Rede Neural Artificial (RNA) utilizando a biblioteca Scikit Learn do Python. O problema escolhido é caracterizado como problema de classificação, onde, através de 11 características (features) de 918 pacientes, deseja-se saber se determinado paciente poderia desenvolver ou não doença cardiovascular (target). Os dados foram obtidos através da plataforma Kaggle (https://www.kaggle.com/fedesoriano/heart-failure-prediction?select=heart.csv) e disponibilizados pela Universidade da Califórnia Irvine.\n","Para o experimento, foi considerado as seguintes condições:\n","1. Dataset de jogos de OverWatch ou outro do Kaggle;\n","2. Dividir o dataset em treino, desenvolvimento e teste. Desenvolvimento será utilizado para encontrar o melhor modelo de rede de acordo com seus parâmetros;\n","3. A rede deve ser treinada no conjunto de treino e calibrada no conjunto de desenvolvimento. Variar os seguintes parâmetros: taxa de aprendizado, número de camadas e função de ativação;\n","4. A calibração deve ser otimizada pela acurácia ou macro-f1;\n","5. Comparar para o conjunto de teste, o melhor modelo de rede neural encontrado contra um classificador por Regressão Logística;\n","6. Seu conjunto de dados deve ser padronizado utilizando z-score. Deve ser feito a geração das estatísticas no treino (fit) e com isso, aplicar a padronização nos conjuntos de treino, desenvolvimento e teste;\n","7. No conjunto de teste, gerar a matriz de confusão e métricas de f1 e acurácia."],"id":"hOG7QdOYbqdq"},{"cell_type":"markdown","metadata":{"id":"6eP9jbSJfFiI"},"source":["### Preparação dos dados"],"id":"6eP9jbSJfFiI"},{"cell_type":"markdown","metadata":{"id":"GXzNnOiMfJF3"},"source":["Algumas features passaram por um pré-processamento para serem utilizadas pelos algoritmos de machine learning. Foo feito o seguinte mapeamento dos dados textuais para o formato de números:\n","1. Sex: M=1, F=2\n","2. ChestPain: ATA=1, NAP=2, ASY=3\n","3. RestingECG: Normal=1, ST=2, LVH=3\n","4. ExerciseA: Y=1, N=0\n","5. ST_Slope: Up=1, Flat=2, Down=3"],"id":"GXzNnOiMfJF3"},{"cell_type":"markdown","metadata":{"id":"YSHGy8SwfQLv"},"source":["### Importando as bibliotecas que serão utilizadas"],"id":"YSHGy8SwfQLv"},{"cell_type":"code","metadata":{"id":"edd30937"},"source":["#Modelos de ML\n","from sklearn.neural_network import MLPClassifier #Multi Layer Percetron Classifier\n","from sklearn.linear_model import LogisticRegression #Regressao Logistica\n","\n","#Metricas e pré-processamento\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay #necessario: matriz de confusao (problemas de classificacao)\n","from sklearn.metrics import accuracy_score #necessario: acuracia\n","from sklearn.metrics import classification_report#necessario: metricas f1\n","from sklearn.metrics import r2_score #erro quadratico\n","from sklearn.model_selection import train_test_split #Separacao dos dados\n","from sklearn.preprocessing import StandardScaler #Z-score\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np"],"id":"edd30937","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sv0G-pzVf3D2"},"source":["### Importando o dataset"],"id":"sv0G-pzVf3D2"},{"cell_type":"code","metadata":{"id":"720c234f"},"source":["#Forma final\n","data = pd.read_csv(\"heart.csv\")"],"id":"720c234f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uKdwgWCGgo_s"},"source":["### Preparando os dados para o desenvolvimento, treinamento e teste\n"],"id":"uKdwgWCGgo_s"},{"cell_type":"code","metadata":{"id":"e54770da"},"source":["x = data.iloc[:, 0:11]\n","y = data[[\"HeartDisease\"]].values.ravel()\n","\n","escala = StandardScaler()\n","escala.fit(x)\n","xNorm = escala.transform(x)\n","\n","#Separação do conjunto de dados para desenvolvimento\n","xRemaining, xDev, yRemaining, yDev = train_test_split(xNorm, y, test_size=0.20)\n","\n","#Dados para desenvolvimento\n","xDevTrain, xDevTest, yDevTrain, yDevTest = train_test_split(xDev, yDev, test_size=0.50)\n","\n","#Dados para treinamento e Teste\n","xTrain, xTest, yTrain, yTest = train_test_split(xRemaining, yRemaining, test_size=0.30)"],"id":"e54770da","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dBwjRhQcmYuN"},"source":["### Desenvolvimento: Parametrizando para encontrar o melhor modelo de"],"id":"dBwjRhQcmYuN"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6eVRaR6mZak","executionInfo":{"elapsed":553,"status":"ok","timestamp":1637353863531,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"1b19c4a8-adbc-49d3-8176-84b602f28118"},"source":["rnaDev = MLPClassifier(hidden_layer_sizes=(8,8,8), #necessario: variar numero camadas\n","                    max_iter=3000,\n","                    tol=0.000000100,\n","                    learning_rate_init=0.01, #necessario: variar taxa de aprendizado\n","                    solver=\"adam\",\n","                    activation=\"relu\", #necessario: variar funcao de ativacao\n","                    learning_rate=\"constant\",\n","                    verbose=True,\n","                    random_state = 0\n","                  )\n","rnaDev.fit(xDevTrain,yDevTrain)"],"id":"h6eVRaR6mZak","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 0.85512281\n","Iteration 2, loss = 0.79846885\n","Iteration 3, loss = 0.74855536\n","Iteration 4, loss = 0.70483537\n","Iteration 5, loss = 0.66959962\n","Iteration 6, loss = 0.64206879\n","Iteration 7, loss = 0.61952552\n","Iteration 8, loss = 0.60129853\n","Iteration 9, loss = 0.58577122\n","Iteration 10, loss = 0.57069541\n","Iteration 11, loss = 0.55468937\n","Iteration 12, loss = 0.53812890\n","Iteration 13, loss = 0.52125769\n","Iteration 14, loss = 0.50433830\n","Iteration 15, loss = 0.48788461\n","Iteration 16, loss = 0.47191435\n","Iteration 17, loss = 0.45586561\n","Iteration 18, loss = 0.44062576\n","Iteration 19, loss = 0.42701665\n","Iteration 20, loss = 0.41459412\n","Iteration 21, loss = 0.40330505\n","Iteration 22, loss = 0.39328513\n","Iteration 23, loss = 0.38456857\n","Iteration 24, loss = 0.37713490\n","Iteration 25, loss = 0.37077659\n","Iteration 26, loss = 0.36432847\n","Iteration 27, loss = 0.35756086\n","Iteration 28, loss = 0.34978704\n","Iteration 29, loss = 0.34171631\n","Iteration 30, loss = 0.33341393\n","Iteration 31, loss = 0.32521826\n","Iteration 32, loss = 0.31830174\n","Iteration 33, loss = 0.31159188\n","Iteration 34, loss = 0.30513821\n","Iteration 35, loss = 0.29870601\n","Iteration 36, loss = 0.29218772\n","Iteration 37, loss = 0.28512204\n","Iteration 38, loss = 0.27741315\n","Iteration 39, loss = 0.26969700\n","Iteration 40, loss = 0.26251385\n","Iteration 41, loss = 0.25608565\n","Iteration 42, loss = 0.25112798\n","Iteration 43, loss = 0.24674422\n","Iteration 44, loss = 0.24224067\n","Iteration 45, loss = 0.23778011\n","Iteration 46, loss = 0.23296618\n","Iteration 47, loss = 0.22768729\n","Iteration 48, loss = 0.22239393\n","Iteration 49, loss = 0.21724831\n","Iteration 50, loss = 0.21227483\n","Iteration 51, loss = 0.20732790\n","Iteration 52, loss = 0.20247667\n","Iteration 53, loss = 0.19751900\n","Iteration 54, loss = 0.19261822\n","Iteration 55, loss = 0.18762684\n","Iteration 56, loss = 0.18256111\n","Iteration 57, loss = 0.17732329\n","Iteration 58, loss = 0.17184535\n","Iteration 59, loss = 0.16619221\n","Iteration 60, loss = 0.16068587\n","Iteration 61, loss = 0.15509889\n","Iteration 62, loss = 0.14933638\n","Iteration 63, loss = 0.14378869\n","Iteration 64, loss = 0.13859867\n","Iteration 65, loss = 0.13363586\n","Iteration 66, loss = 0.12897538\n","Iteration 67, loss = 0.12447208\n","Iteration 68, loss = 0.12051144\n","Iteration 69, loss = 0.11655969\n","Iteration 70, loss = 0.11243020\n","Iteration 71, loss = 0.10825072\n","Iteration 72, loss = 0.10417233\n","Iteration 73, loss = 0.10029424\n","Iteration 74, loss = 0.09660962\n","Iteration 75, loss = 0.09307447\n","Iteration 76, loss = 0.08976939\n","Iteration 77, loss = 0.08647040\n","Iteration 78, loss = 0.08307839\n","Iteration 79, loss = 0.07983750\n","Iteration 80, loss = 0.07693784\n","Iteration 81, loss = 0.07433067\n","Iteration 82, loss = 0.07172442\n","Iteration 83, loss = 0.06906887\n","Iteration 84, loss = 0.06658089\n","Iteration 85, loss = 0.06450548\n","Iteration 86, loss = 0.06255626\n","Iteration 87, loss = 0.06094927\n","Iteration 88, loss = 0.05940186\n","Iteration 89, loss = 0.05788010\n","Iteration 90, loss = 0.05637425\n","Iteration 91, loss = 0.05489732\n","Iteration 92, loss = 0.05352634\n","Iteration 93, loss = 0.05218151\n","Iteration 94, loss = 0.05082683\n","Iteration 95, loss = 0.04873909\n","Iteration 96, loss = 0.04620210\n","Iteration 97, loss = 0.04316822\n","Iteration 98, loss = 0.04007429\n","Iteration 99, loss = 0.03790981\n","Iteration 100, loss = 0.03635467\n","Iteration 101, loss = 0.03489618\n","Iteration 102, loss = 0.03343449\n","Iteration 103, loss = 0.03245171\n","Iteration 104, loss = 0.03154204\n","Iteration 105, loss = 0.03074645\n","Iteration 106, loss = 0.03001097\n","Iteration 107, loss = 0.02932551\n","Iteration 108, loss = 0.02870923\n","Iteration 109, loss = 0.02810184\n","Iteration 110, loss = 0.02750314\n","Iteration 111, loss = 0.02694139\n","Iteration 112, loss = 0.02642258\n","Iteration 113, loss = 0.02597070\n","Iteration 114, loss = 0.02556231\n","Iteration 115, loss = 0.02518213\n","Iteration 116, loss = 0.02482261\n","Iteration 117, loss = 0.02447843\n","Iteration 118, loss = 0.02416076\n","Iteration 119, loss = 0.02380108\n","Iteration 120, loss = 0.02304536\n","Iteration 121, loss = 0.02222925\n","Iteration 122, loss = 0.01960959\n","Iteration 123, loss = 0.01822386\n","Iteration 124, loss = 0.01791833\n","Iteration 125, loss = 0.01754591\n","Iteration 126, loss = 0.01619813\n","Iteration 127, loss = 0.01435827\n","Iteration 128, loss = 0.01293069\n","Iteration 129, loss = 0.01238478\n","Iteration 130, loss = 0.01183920\n","Iteration 131, loss = 0.01106365\n","Iteration 132, loss = 0.01005526\n","Iteration 133, loss = 0.00917606\n","Iteration 134, loss = 0.00864357\n","Iteration 135, loss = 0.00831065\n","Iteration 136, loss = 0.00794210\n","Iteration 137, loss = 0.00747314\n","Iteration 138, loss = 0.00696890\n","Iteration 139, loss = 0.00664462\n","Iteration 140, loss = 0.00632703\n","Iteration 141, loss = 0.00602392\n","Iteration 142, loss = 0.00574097\n","Iteration 143, loss = 0.00547904\n","Iteration 144, loss = 0.00523472\n","Iteration 145, loss = 0.00500579\n","Iteration 146, loss = 0.00478967\n","Iteration 147, loss = 0.00458414\n","Iteration 148, loss = 0.00438980\n","Iteration 149, loss = 0.00420455\n","Iteration 150, loss = 0.00402759\n","Iteration 151, loss = 0.00385880\n","Iteration 152, loss = 0.00369852\n","Iteration 153, loss = 0.00354739\n","Iteration 154, loss = 0.00340414\n","Iteration 155, loss = 0.00326848\n","Iteration 156, loss = 0.00314001\n","Iteration 157, loss = 0.00301762\n","Iteration 158, loss = 0.00290023\n","Iteration 159, loss = 0.00278782\n","Iteration 160, loss = 0.00268048\n","Iteration 161, loss = 0.00257835\n","Iteration 162, loss = 0.00248225\n","Iteration 163, loss = 0.00239168\n","Iteration 164, loss = 0.00230610\n","Iteration 165, loss = 0.00222526\n","Iteration 166, loss = 0.00214889\n","Iteration 167, loss = 0.00207643\n","Iteration 168, loss = 0.00200749\n","Iteration 169, loss = 0.00194189\n","Iteration 170, loss = 0.00187998\n","Iteration 171, loss = 0.00182103\n","Iteration 172, loss = 0.00176512\n","Iteration 173, loss = 0.00171232\n","Iteration 174, loss = 0.00166200\n","Iteration 175, loss = 0.00161404\n","Iteration 176, loss = 0.00156834\n","Iteration 177, loss = 0.00152478\n","Iteration 178, loss = 0.00148330\n","Iteration 179, loss = 0.00144387\n","Iteration 180, loss = 0.00140623\n","Iteration 181, loss = 0.00137022\n","Iteration 182, loss = 0.00133573\n","Iteration 183, loss = 0.00130271\n","Iteration 184, loss = 0.00127127\n","Iteration 185, loss = 0.00124117\n","Iteration 186, loss = 0.00121220\n","Iteration 187, loss = 0.00118436\n","Iteration 188, loss = 0.00115768\n","Iteration 189, loss = 0.00113203\n","Iteration 190, loss = 0.00110737\n","Iteration 191, loss = 0.00108363\n","Iteration 192, loss = 0.00106080\n","Iteration 193, loss = 0.00103907\n","Iteration 194, loss = 0.00101812\n","Iteration 195, loss = 0.00099785\n","Iteration 196, loss = 0.00097822\n","Iteration 197, loss = 0.00095944\n","Iteration 198, loss = 0.00094139\n","Iteration 199, loss = 0.00092429\n","Iteration 200, loss = 0.00090770\n","Iteration 201, loss = 0.00089146\n","Iteration 202, loss = 0.00087559\n","Iteration 203, loss = 0.00086014\n","Iteration 204, loss = 0.00084529\n","Iteration 205, loss = 0.00083086\n","Iteration 206, loss = 0.00081694\n","Iteration 207, loss = 0.00080348\n","Iteration 208, loss = 0.00079038\n","Iteration 209, loss = 0.00077764\n","Iteration 210, loss = 0.00076537\n","Iteration 211, loss = 0.00075348\n","Iteration 212, loss = 0.00074186\n","Iteration 213, loss = 0.00073055\n","Iteration 214, loss = 0.00071948\n","Iteration 215, loss = 0.00070883\n","Iteration 216, loss = 0.00069853\n","Iteration 217, loss = 0.00068851\n","Iteration 218, loss = 0.00067876\n","Iteration 219, loss = 0.00066920\n","Iteration 220, loss = 0.00065983\n","Iteration 221, loss = 0.00065068\n","Iteration 222, loss = 0.00064173\n","Iteration 223, loss = 0.00063305\n","Iteration 224, loss = 0.00062461\n","Iteration 225, loss = 0.00061634\n","Iteration 226, loss = 0.00060834\n","Iteration 227, loss = 0.00060051\n","Iteration 228, loss = 0.00059284\n","Iteration 229, loss = 0.00058536\n","Iteration 230, loss = 0.00057810\n","Iteration 231, loss = 0.00057109\n","Iteration 232, loss = 0.00056402\n","Iteration 233, loss = 0.00055720\n","Iteration 234, loss = 0.00055061\n","Iteration 235, loss = 0.00054413\n","Iteration 236, loss = 0.00053776\n","Iteration 237, loss = 0.00053153\n","Iteration 238, loss = 0.00052542\n","Iteration 239, loss = 0.00051966\n","Iteration 240, loss = 0.00051365\n","Iteration 241, loss = 0.00050795\n","Iteration 242, loss = 0.00050240\n","Iteration 243, loss = 0.00049693\n","Iteration 244, loss = 0.00049152\n","Iteration 245, loss = 0.00048630\n","Iteration 246, loss = 0.00048110\n","Iteration 247, loss = 0.00047597\n","Iteration 248, loss = 0.00047098\n","Iteration 249, loss = 0.00046607\n","Iteration 250, loss = 0.00046137\n","Iteration 251, loss = 0.00045662\n","Iteration 252, loss = 0.00045202\n","Iteration 253, loss = 0.00044752\n","Iteration 254, loss = 0.00044308\n","Iteration 255, loss = 0.00043871\n","Iteration 256, loss = 0.00043440\n","Iteration 257, loss = 0.00043019\n","Iteration 258, loss = 0.00042604\n","Iteration 259, loss = 0.00042200\n","Iteration 260, loss = 0.00041805\n","Iteration 261, loss = 0.00041413\n","Iteration 262, loss = 0.00041027\n","Iteration 263, loss = 0.00040648\n","Iteration 264, loss = 0.00040276\n","Iteration 265, loss = 0.00039907\n","Iteration 266, loss = 0.00039543\n","Iteration 267, loss = 0.00039185\n","Iteration 268, loss = 0.00038852\n","Iteration 269, loss = 0.00038493\n","Iteration 270, loss = 0.00038156\n","Iteration 271, loss = 0.00037822\n","Iteration 272, loss = 0.00037498\n","Iteration 273, loss = 0.00037178\n","Iteration 274, loss = 0.00036861\n","Iteration 275, loss = 0.00036552\n","Iteration 276, loss = 0.00036248\n","Iteration 277, loss = 0.00035950\n","Iteration 278, loss = 0.00035658\n","Iteration 279, loss = 0.00035367\n","Iteration 280, loss = 0.00035082\n","Iteration 281, loss = 0.00034799\n","Iteration 282, loss = 0.00034521\n","Iteration 283, loss = 0.00034244\n","Iteration 284, loss = 0.00033974\n","Iteration 285, loss = 0.00033709\n","Iteration 286, loss = 0.00033457\n","Iteration 287, loss = 0.00033196\n","Iteration 288, loss = 0.00032945\n","Iteration 289, loss = 0.00032696\n","Iteration 290, loss = 0.00032449\n","Iteration 291, loss = 0.00032203\n","Iteration 292, loss = 0.00031971\n","Iteration 293, loss = 0.00031728\n","Iteration 294, loss = 0.00031495\n","Iteration 295, loss = 0.00031265\n","Iteration 296, loss = 0.00031040\n","Iteration 297, loss = 0.00030824\n","Iteration 298, loss = 0.00030600\n","Iteration 299, loss = 0.00030384\n"]},{"name":"stdout","output_type":"stream","text":["Iteration 300, loss = 0.00030173\n","Iteration 301, loss = 0.00029964\n","Iteration 302, loss = 0.00029755\n","Iteration 303, loss = 0.00029552\n","Iteration 304, loss = 0.00029352\n","Iteration 305, loss = 0.00029155\n","Iteration 306, loss = 0.00028958\n","Iteration 307, loss = 0.00028764\n","Iteration 308, loss = 0.00028573\n","Iteration 309, loss = 0.00028384\n","Iteration 310, loss = 0.00028196\n","Iteration 311, loss = 0.00028008\n","Iteration 312, loss = 0.00027825\n","Iteration 313, loss = 0.00027642\n","Iteration 314, loss = 0.00027464\n","Iteration 315, loss = 0.00027284\n","Iteration 316, loss = 0.00027129\n","Iteration 317, loss = 0.00026946\n","Iteration 318, loss = 0.00026780\n","Iteration 319, loss = 0.00026616\n","Iteration 320, loss = 0.00026452\n","Iteration 321, loss = 0.00026289\n","Iteration 322, loss = 0.00026127\n","Iteration 323, loss = 0.00025966\n","Iteration 324, loss = 0.00025808\n","Iteration 325, loss = 0.00025649\n","Iteration 326, loss = 0.00025492\n","Iteration 327, loss = 0.00025339\n","Iteration 328, loss = 0.00025187\n","Iteration 329, loss = 0.00025034\n","Iteration 330, loss = 0.00024885\n","Iteration 331, loss = 0.00024738\n","Iteration 332, loss = 0.00024594\n","Iteration 333, loss = 0.00024450\n","Iteration 334, loss = 0.00024309\n","Iteration 335, loss = 0.00024168\n","Iteration 336, loss = 0.00024029\n","Iteration 337, loss = 0.00023892\n","Iteration 338, loss = 0.00023766\n","Iteration 339, loss = 0.00023624\n","Iteration 340, loss = 0.00023494\n","Iteration 341, loss = 0.00023363\n","Iteration 342, loss = 0.00023232\n","Iteration 343, loss = 0.00023103\n","Iteration 344, loss = 0.00022976\n","Iteration 345, loss = 0.00022849\n","Iteration 346, loss = 0.00022724\n","Iteration 347, loss = 0.00022605\n","Iteration 348, loss = 0.00022480\n","Iteration 349, loss = 0.00022360\n","Iteration 350, loss = 0.00022241\n","Iteration 351, loss = 0.00022122\n","Iteration 352, loss = 0.00022004\n","Iteration 353, loss = 0.00021889\n","Iteration 354, loss = 0.00021775\n","Iteration 355, loss = 0.00021664\n","Iteration 356, loss = 0.00021553\n","Iteration 357, loss = 0.00021442\n","Iteration 358, loss = 0.00021331\n","Iteration 359, loss = 0.00021220\n","Iteration 360, loss = 0.00021112\n","Iteration 361, loss = 0.00021006\n","Iteration 362, loss = 0.00020899\n","Iteration 363, loss = 0.00020794\n","Iteration 364, loss = 0.00020691\n","Iteration 365, loss = 0.00020590\n","Iteration 366, loss = 0.00020490\n","Iteration 367, loss = 0.00020390\n","Iteration 368, loss = 0.00020290\n","Iteration 369, loss = 0.00020190\n","Iteration 370, loss = 0.00020090\n","Iteration 371, loss = 0.00019994\n","Iteration 372, loss = 0.00019896\n","Iteration 373, loss = 0.00019801\n","Iteration 374, loss = 0.00019708\n","Iteration 375, loss = 0.00019615\n","Iteration 376, loss = 0.00019522\n","Iteration 377, loss = 0.00019431\n","Iteration 378, loss = 0.00019340\n","Iteration 379, loss = 0.00019249\n","Iteration 380, loss = 0.00019162\n","Iteration 381, loss = 0.00019074\n","Iteration 382, loss = 0.00018985\n","Iteration 383, loss = 0.00018896\n","Iteration 384, loss = 0.00018811\n","Iteration 385, loss = 0.00018726\n","Iteration 386, loss = 0.00018639\n","Iteration 387, loss = 0.00018556\n","Iteration 388, loss = 0.00018473\n","Iteration 389, loss = 0.00018390\n","Iteration 390, loss = 0.00018313\n","Iteration 391, loss = 0.00018232\n","Iteration 392, loss = 0.00018150\n","Iteration 393, loss = 0.00018072\n","Iteration 394, loss = 0.00017992\n","Iteration 395, loss = 0.00017913\n","Iteration 396, loss = 0.00017835\n","Iteration 397, loss = 0.00017758\n","Iteration 398, loss = 0.00017682\n","Iteration 399, loss = 0.00017609\n","Iteration 400, loss = 0.00017532\n","Iteration 401, loss = 0.00017459\n","Iteration 402, loss = 0.00017386\n","Iteration 403, loss = 0.00017313\n","Iteration 404, loss = 0.00017240\n","Iteration 405, loss = 0.00017168\n","Iteration 406, loss = 0.00017095\n","Iteration 407, loss = 0.00017026\n","Iteration 408, loss = 0.00016955\n","Iteration 409, loss = 0.00016885\n","Iteration 410, loss = 0.00016817\n","Iteration 411, loss = 0.00016749\n","Iteration 412, loss = 0.00016682\n","Iteration 413, loss = 0.00016615\n","Iteration 414, loss = 0.00016547\n","Iteration 415, loss = 0.00016484\n","Iteration 416, loss = 0.00016415\n","Iteration 417, loss = 0.00016351\n","Iteration 418, loss = 0.00016288\n","Iteration 419, loss = 0.00016223\n","Iteration 420, loss = 0.00016159\n","Iteration 421, loss = 0.00016097\n","Iteration 422, loss = 0.00016034\n","Iteration 423, loss = 0.00015976\n","Iteration 424, loss = 0.00015912\n","Iteration 425, loss = 0.00015851\n","Iteration 426, loss = 0.00015790\n","Iteration 427, loss = 0.00015730\n","Iteration 428, loss = 0.00015671\n","Iteration 429, loss = 0.00015614\n","Iteration 430, loss = 0.00015553\n","Iteration 431, loss = 0.00015497\n","Iteration 432, loss = 0.00015439\n","Iteration 433, loss = 0.00015382\n","Iteration 434, loss = 0.00015325\n","Iteration 435, loss = 0.00015269\n","Iteration 436, loss = 0.00015212\n","Iteration 437, loss = 0.00015157\n","Iteration 438, loss = 0.00015105\n","Iteration 439, loss = 0.00015047\n","Iteration 440, loss = 0.00014994\n","Iteration 441, loss = 0.00014940\n","Iteration 442, loss = 0.00014886\n","Iteration 443, loss = 0.00014833\n","Iteration 444, loss = 0.00014781\n","Iteration 445, loss = 0.00014728\n","Iteration 446, loss = 0.00014677\n","Iteration 447, loss = 0.00014626\n","Iteration 448, loss = 0.00014575\n","Iteration 449, loss = 0.00014524\n","Iteration 450, loss = 0.00014474\n","Iteration 451, loss = 0.00014425\n","Iteration 452, loss = 0.00014375\n","Iteration 453, loss = 0.00014326\n","Iteration 454, loss = 0.00014277\n","Iteration 455, loss = 0.00014228\n","Iteration 456, loss = 0.00014180\n","Iteration 457, loss = 0.00014133\n","Iteration 458, loss = 0.00014085\n","Iteration 459, loss = 0.00014037\n","Iteration 460, loss = 0.00013990\n","Iteration 461, loss = 0.00013946\n","Iteration 462, loss = 0.00013899\n","Iteration 463, loss = 0.00013854\n","Iteration 464, loss = 0.00013809\n","Iteration 465, loss = 0.00013765\n","Iteration 466, loss = 0.00013720\n","Iteration 467, loss = 0.00013675\n","Iteration 468, loss = 0.00013631\n","Iteration 469, loss = 0.00013587\n","Iteration 470, loss = 0.00013544\n","Iteration 471, loss = 0.00013501\n","Iteration 472, loss = 0.00013458\n","Iteration 473, loss = 0.00013414\n","Iteration 474, loss = 0.00013371\n","Iteration 475, loss = 0.00013330\n","Iteration 476, loss = 0.00013288\n","Iteration 477, loss = 0.00013247\n","Iteration 478, loss = 0.00013206\n","Iteration 479, loss = 0.00013165\n","Iteration 480, loss = 0.00013124\n","Iteration 481, loss = 0.00013083\n","Iteration 482, loss = 0.00013043\n","Iteration 483, loss = 0.00013004\n","Iteration 484, loss = 0.00012964\n","Iteration 485, loss = 0.00012925\n","Iteration 486, loss = 0.00012887\n","Iteration 487, loss = 0.00012848\n","Iteration 488, loss = 0.00012810\n","Iteration 489, loss = 0.00012772\n","Iteration 490, loss = 0.00012734\n","Iteration 491, loss = 0.00012696\n","Iteration 492, loss = 0.00012658\n","Iteration 493, loss = 0.00012621\n","Iteration 494, loss = 0.00012584\n","Iteration 495, loss = 0.00012547\n","Iteration 496, loss = 0.00012510\n","Iteration 497, loss = 0.00012475\n","Iteration 498, loss = 0.00012439\n","Iteration 499, loss = 0.00012403\n","Iteration 500, loss = 0.00012368\n","Iteration 501, loss = 0.00012332\n","Iteration 502, loss = 0.00012297\n","Iteration 503, loss = 0.00012262\n","Iteration 504, loss = 0.00012227\n","Iteration 505, loss = 0.00012193\n","Iteration 506, loss = 0.00012159\n","Iteration 507, loss = 0.00012125\n","Iteration 508, loss = 0.00012092\n","Iteration 509, loss = 0.00012059\n","Iteration 510, loss = 0.00012026\n","Iteration 511, loss = 0.00011993\n","Iteration 512, loss = 0.00011960\n","Iteration 513, loss = 0.00011927\n","Iteration 514, loss = 0.00011894\n","Iteration 515, loss = 0.00011862\n","Iteration 516, loss = 0.00011831\n","Iteration 517, loss = 0.00011798\n","Iteration 518, loss = 0.00011767\n","Iteration 519, loss = 0.00011736\n","Iteration 520, loss = 0.00011705\n","Iteration 521, loss = 0.00011673\n","Iteration 522, loss = 0.00011642\n","Iteration 523, loss = 0.00011612\n","Iteration 524, loss = 0.00011582\n","Iteration 525, loss = 0.00011551\n","Iteration 526, loss = 0.00011521\n","Iteration 527, loss = 0.00011490\n","Iteration 528, loss = 0.00011461\n","Iteration 529, loss = 0.00011431\n","Iteration 530, loss = 0.00011401\n","Iteration 531, loss = 0.00011372\n","Iteration 532, loss = 0.00011343\n","Iteration 533, loss = 0.00011315\n","Iteration 534, loss = 0.00011287\n","Iteration 535, loss = 0.00011258\n","Iteration 536, loss = 0.00011229\n","Iteration 537, loss = 0.00011201\n","Iteration 538, loss = 0.00011173\n","Iteration 539, loss = 0.00011145\n","Iteration 540, loss = 0.00011118\n","Iteration 541, loss = 0.00011090\n","Iteration 542, loss = 0.00011062\n","Iteration 543, loss = 0.00011037\n","Iteration 544, loss = 0.00011009\n","Iteration 545, loss = 0.00010982\n","Iteration 546, loss = 0.00010956\n","Iteration 547, loss = 0.00010930\n","Iteration 548, loss = 0.00010903\n","Iteration 549, loss = 0.00010877\n","Iteration 550, loss = 0.00010851\n","Iteration 551, loss = 0.00010825\n","Iteration 552, loss = 0.00010800\n","Iteration 553, loss = 0.00010773\n","Iteration 554, loss = 0.00010748\n","Iteration 555, loss = 0.00010722\n","Iteration 556, loss = 0.00010697\n","Iteration 557, loss = 0.00010673\n","Iteration 558, loss = 0.00010648\n","Iteration 559, loss = 0.00010623\n","Iteration 560, loss = 0.00010599\n","Iteration 561, loss = 0.00010575\n","Iteration 562, loss = 0.00010550\n","Iteration 563, loss = 0.00010525\n","Iteration 564, loss = 0.00010501\n","Iteration 565, loss = 0.00010477\n","Iteration 566, loss = 0.00010455\n","Iteration 567, loss = 0.00010429\n","Iteration 568, loss = 0.00010406\n","Iteration 569, loss = 0.00010383\n","Iteration 570, loss = 0.00010360\n","Iteration 571, loss = 0.00010337\n","Iteration 572, loss = 0.00010314\n","Iteration 573, loss = 0.00010290\n","Iteration 574, loss = 0.00010268\n","Iteration 575, loss = 0.00010245\n","Iteration 576, loss = 0.00010223\n","Iteration 577, loss = 0.00010200\n","Iteration 578, loss = 0.00010178\n","Iteration 579, loss = 0.00010156\n","Iteration 580, loss = 0.00010134\n","Iteration 581, loss = 0.00010112\n","Iteration 582, loss = 0.00010090\n","Iteration 583, loss = 0.00010069\n","Iteration 584, loss = 0.00010047\n","Iteration 585, loss = 0.00010026\n","Iteration 586, loss = 0.00010005\n","Iteration 587, loss = 0.00009984\n","Iteration 588, loss = 0.00009963\n","Iteration 589, loss = 0.00009942\n","Iteration 590, loss = 0.00009921\n","Iteration 591, loss = 0.00009901\n","Iteration 592, loss = 0.00009880\n","Iteration 593, loss = 0.00009859\n","Iteration 594, loss = 0.00009838\n","Iteration 595, loss = 0.00009819\n","Iteration 596, loss = 0.00009799\n","Iteration 597, loss = 0.00009779\n","Iteration 598, loss = 0.00009759\n","Iteration 599, loss = 0.00009739\n","Iteration 600, loss = 0.00009719\n","Iteration 601, loss = 0.00009700\n","Iteration 602, loss = 0.00009680\n","Iteration 603, loss = 0.00009661\n","Iteration 604, loss = 0.00009642\n","Iteration 605, loss = 0.00009622\n","Iteration 606, loss = 0.00009603\n","Iteration 607, loss = 0.00009584\n","Iteration 608, loss = 0.00009565\n","Iteration 609, loss = 0.00009546\n","Iteration 610, loss = 0.00009529\n","Iteration 611, loss = 0.00009509\n","Iteration 612, loss = 0.00009490\n","Iteration 613, loss = 0.00009472\n","Iteration 614, loss = 0.00009454\n","Iteration 615, loss = 0.00009436\n","Iteration 616, loss = 0.00009418\n","Iteration 617, loss = 0.00009400\n","Iteration 618, loss = 0.00009381\n","Iteration 619, loss = 0.00009364\n"]},{"name":"stdout","output_type":"stream","text":["Iteration 620, loss = 0.00009346\n","Iteration 621, loss = 0.00009329\n","Iteration 622, loss = 0.00009311\n","Iteration 623, loss = 0.00009294\n","Iteration 624, loss = 0.00009277\n","Iteration 625, loss = 0.00009259\n","Iteration 626, loss = 0.00009242\n","Iteration 627, loss = 0.00009224\n","Iteration 628, loss = 0.00009208\n","Iteration 629, loss = 0.00009191\n","Iteration 630, loss = 0.00009173\n","Iteration 631, loss = 0.00009157\n","Iteration 632, loss = 0.00009140\n","Iteration 633, loss = 0.00009123\n","Iteration 634, loss = 0.00009107\n","Iteration 635, loss = 0.00009091\n","Iteration 636, loss = 0.00009074\n","Iteration 637, loss = 0.00009058\n","Iteration 638, loss = 0.00009041\n","Iteration 639, loss = 0.00009025\n","Iteration 640, loss = 0.00009009\n","Iteration 641, loss = 0.00008993\n","Iteration 642, loss = 0.00008977\n","Iteration 643, loss = 0.00008962\n","Iteration 644, loss = 0.00008946\n","Iteration 645, loss = 0.00008930\n","Iteration 646, loss = 0.00008915\n","Iteration 647, loss = 0.00008899\n","Iteration 648, loss = 0.00008883\n","Iteration 649, loss = 0.00008868\n","Iteration 650, loss = 0.00008853\n","Iteration 651, loss = 0.00008838\n","Iteration 652, loss = 0.00008822\n","Iteration 653, loss = 0.00008807\n","Iteration 654, loss = 0.00008792\n","Iteration 655, loss = 0.00008777\n","Iteration 656, loss = 0.00008762\n","Iteration 657, loss = 0.00008748\n","Iteration 658, loss = 0.00008733\n","Iteration 659, loss = 0.00008718\n","Iteration 660, loss = 0.00008704\n","Iteration 661, loss = 0.00008689\n","Iteration 662, loss = 0.00008675\n","Iteration 663, loss = 0.00008661\n","Iteration 664, loss = 0.00008646\n","Iteration 665, loss = 0.00008632\n","Iteration 666, loss = 0.00008618\n","Iteration 667, loss = 0.00008604\n","Iteration 668, loss = 0.00008589\n","Iteration 669, loss = 0.00008576\n","Iteration 670, loss = 0.00008562\n","Iteration 671, loss = 0.00008548\n","Iteration 672, loss = 0.00008534\n","Iteration 673, loss = 0.00008521\n","Iteration 674, loss = 0.00008507\n","Iteration 675, loss = 0.00008493\n","Iteration 676, loss = 0.00008480\n","Iteration 677, loss = 0.00008466\n","Iteration 678, loss = 0.00008453\n","Iteration 679, loss = 0.00008439\n","Iteration 680, loss = 0.00008426\n","Iteration 681, loss = 0.00008413\n","Iteration 682, loss = 0.00008400\n","Iteration 683, loss = 0.00008387\n","Iteration 684, loss = 0.00008374\n","Iteration 685, loss = 0.00008361\n","Iteration 686, loss = 0.00008348\n","Iteration 687, loss = 0.00008335\n","Iteration 688, loss = 0.00008323\n","Iteration 689, loss = 0.00008310\n","Iteration 690, loss = 0.00008297\n","Iteration 691, loss = 0.00008284\n","Iteration 692, loss = 0.00008271\n","Iteration 693, loss = 0.00008259\n","Iteration 694, loss = 0.00008247\n","Iteration 695, loss = 0.00008235\n","Iteration 696, loss = 0.00008222\n","Iteration 697, loss = 0.00008210\n","Iteration 698, loss = 0.00008197\n","Iteration 699, loss = 0.00008185\n","Iteration 700, loss = 0.00008173\n","Iteration 701, loss = 0.00008161\n","Iteration 702, loss = 0.00008149\n","Iteration 703, loss = 0.00008137\n","Iteration 704, loss = 0.00008125\n","Iteration 705, loss = 0.00008114\n","Iteration 706, loss = 0.00008102\n","Iteration 707, loss = 0.00008090\n","Iteration 708, loss = 0.00008078\n","Iteration 709, loss = 0.00008067\n","Iteration 710, loss = 0.00008055\n","Iteration 711, loss = 0.00008043\n","Iteration 712, loss = 0.00008032\n","Iteration 713, loss = 0.00008021\n","Iteration 714, loss = 0.00008009\n","Iteration 715, loss = 0.00007998\n","Iteration 716, loss = 0.00007987\n","Iteration 717, loss = 0.00007975\n","Iteration 718, loss = 0.00007964\n","Iteration 719, loss = 0.00007953\n","Iteration 720, loss = 0.00007942\n","Iteration 721, loss = 0.00007931\n","Iteration 722, loss = 0.00007920\n","Iteration 723, loss = 0.00007909\n","Iteration 724, loss = 0.00007898\n","Iteration 725, loss = 0.00007887\n","Iteration 726, loss = 0.00007876\n","Iteration 727, loss = 0.00007866\n","Iteration 728, loss = 0.00007855\n","Iteration 729, loss = 0.00007844\n","Iteration 730, loss = 0.00007834\n","Iteration 731, loss = 0.00007823\n","Iteration 732, loss = 0.00007812\n","Iteration 733, loss = 0.00007802\n","Iteration 734, loss = 0.00007791\n","Iteration 735, loss = 0.00007781\n","Iteration 736, loss = 0.00007771\n","Iteration 737, loss = 0.00007761\n","Iteration 738, loss = 0.00007750\n","Iteration 739, loss = 0.00007740\n","Iteration 740, loss = 0.00007730\n","Iteration 741, loss = 0.00007720\n","Iteration 742, loss = 0.00007710\n","Iteration 743, loss = 0.00007700\n","Iteration 744, loss = 0.00007690\n","Iteration 745, loss = 0.00007680\n","Iteration 746, loss = 0.00007670\n","Iteration 747, loss = 0.00007660\n","Iteration 748, loss = 0.00007650\n","Iteration 749, loss = 0.00007640\n","Iteration 750, loss = 0.00007630\n","Iteration 751, loss = 0.00007620\n","Iteration 752, loss = 0.00007611\n","Iteration 753, loss = 0.00007601\n","Iteration 754, loss = 0.00007592\n","Iteration 755, loss = 0.00007582\n","Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"]},{"data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(8, 8, 8), learning_rate_init=0.01,\n","              max_iter=3000, random_state=0, tol=1e-07, verbose=True)"]},"execution_count":456,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndCO9KZjpl2Q","executionInfo":{"elapsed":5,"status":"ok","timestamp":1637353863744,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"c9519e59-cff5-4faa-f0a7-075206237af5"},"source":["rnaDevPredicted = rnaDev.predict(xDevTest)\n","print(\"Acurácia: \", accuracy_score(yDevTest, rnaDevPredicted))"],"id":"ndCO9KZjpl2Q","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Acurácia:  0.7934782608695652\n"]}]},{"cell_type":"markdown","metadata":{"id":"5FXnzQyCpbiY"},"source":["### Treinamento: O modelo com a parametrização anterior"],"id":"5FXnzQyCpbiY"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99fd793b","executionInfo":{"elapsed":850,"status":"ok","timestamp":1637353867561,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"f04afee8-31b2-413a-9de8-33ebdfeac2b2"},"source":["rna = MLPClassifier(hidden_layer_sizes=(8,8,8), #necessario: variar numero camadas\n","                    max_iter=3000,\n","                    tol=0.000000100,\n","                    learning_rate_init=0.01,\n","                    solver=\"adam\",\n","                    activation=\"relu\",\n","                    learning_rate=\"constant\",\n","                    verbose=True,\n","                  )\n","rna.fit(xTrain,yTrain)\n"],"id":"99fd793b","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 0.74680315\n","Iteration 2, loss = 0.66785467\n","Iteration 3, loss = 0.62575660\n","Iteration 4, loss = 0.58874155\n","Iteration 5, loss = 0.54885391\n","Iteration 6, loss = 0.50978346\n","Iteration 7, loss = 0.48004691\n","Iteration 8, loss = 0.45669941\n","Iteration 9, loss = 0.43913601\n","Iteration 10, loss = 0.42217096\n","Iteration 11, loss = 0.40716754\n","Iteration 12, loss = 0.39779787\n","Iteration 13, loss = 0.38776241\n","Iteration 14, loss = 0.37840069\n","Iteration 15, loss = 0.36908970\n","Iteration 16, loss = 0.36200732\n","Iteration 17, loss = 0.35550920\n","Iteration 18, loss = 0.34984549\n","Iteration 19, loss = 0.34367372\n","Iteration 20, loss = 0.34078124\n","Iteration 21, loss = 0.33540499\n","Iteration 22, loss = 0.32996479\n","Iteration 23, loss = 0.32585699\n","Iteration 24, loss = 0.32059170\n","Iteration 25, loss = 0.31593836\n","Iteration 26, loss = 0.31167559\n","Iteration 27, loss = 0.30708820\n","Iteration 28, loss = 0.30311538\n","Iteration 29, loss = 0.29940197\n","Iteration 30, loss = 0.29524433\n","Iteration 31, loss = 0.29259507\n","Iteration 32, loss = 0.28963385\n","Iteration 33, loss = 0.28667121\n","Iteration 34, loss = 0.28368335\n","Iteration 35, loss = 0.28054965\n","Iteration 36, loss = 0.27863043\n","Iteration 37, loss = 0.27547593\n","Iteration 38, loss = 0.27229911\n","Iteration 39, loss = 0.26971942\n","Iteration 40, loss = 0.26748755\n","Iteration 41, loss = 0.26495178\n","Iteration 42, loss = 0.26263052\n","Iteration 43, loss = 0.26036596\n","Iteration 44, loss = 0.25776973\n","Iteration 45, loss = 0.25531254\n","Iteration 46, loss = 0.25230163\n","Iteration 47, loss = 0.24952735\n","Iteration 48, loss = 0.24986733\n","Iteration 49, loss = 0.24563922\n","Iteration 50, loss = 0.24456989\n","Iteration 51, loss = 0.23944764\n","Iteration 52, loss = 0.23804730\n","Iteration 53, loss = 0.23444976\n","Iteration 54, loss = 0.23297702\n","Iteration 55, loss = 0.23055381\n","Iteration 56, loss = 0.22433037\n","Iteration 57, loss = 0.22093642\n","Iteration 58, loss = 0.21858283\n","Iteration 59, loss = 0.21585973\n","Iteration 60, loss = 0.21005537\n","Iteration 61, loss = 0.20808447\n","Iteration 62, loss = 0.20324357\n","Iteration 63, loss = 0.20217881\n","Iteration 64, loss = 0.19703833\n","Iteration 65, loss = 0.19332722\n","Iteration 66, loss = 0.18993378\n","Iteration 67, loss = 0.18470909\n","Iteration 68, loss = 0.18131945\n","Iteration 69, loss = 0.17671543\n","Iteration 70, loss = 0.17552608\n","Iteration 71, loss = 0.17061569\n","Iteration 72, loss = 0.16884172\n","Iteration 73, loss = 0.16439943\n","Iteration 74, loss = 0.16211098\n","Iteration 75, loss = 0.15979465\n","Iteration 76, loss = 0.15755163\n","Iteration 77, loss = 0.15274679\n","Iteration 78, loss = 0.14969483\n","Iteration 79, loss = 0.14838124\n","Iteration 80, loss = 0.14399869\n","Iteration 81, loss = 0.14188553\n","Iteration 82, loss = 0.14174426\n","Iteration 83, loss = 0.13736157\n","Iteration 84, loss = 0.13558795\n","Iteration 85, loss = 0.13520698\n","Iteration 86, loss = 0.13264548\n","Iteration 87, loss = 0.13133186\n","Iteration 88, loss = 0.12808026\n","Iteration 89, loss = 0.12702643\n","Iteration 90, loss = 0.12445762\n","Iteration 91, loss = 0.12635911\n","Iteration 92, loss = 0.12147318\n","Iteration 93, loss = 0.12262964\n","Iteration 94, loss = 0.12293298\n","Iteration 95, loss = 0.11988656\n","Iteration 96, loss = 0.11934571\n","Iteration 97, loss = 0.11396703\n","Iteration 98, loss = 0.11273745\n","Iteration 99, loss = 0.11214718\n","Iteration 100, loss = 0.11295297\n","Iteration 101, loss = 0.10884031\n","Iteration 102, loss = 0.10854436\n","Iteration 103, loss = 0.10602378\n","Iteration 104, loss = 0.10842760\n","Iteration 105, loss = 0.10592603\n","Iteration 106, loss = 0.10747321\n","Iteration 107, loss = 0.10974486\n","Iteration 108, loss = 0.10948900\n","Iteration 109, loss = 0.10377954\n","Iteration 110, loss = 0.10306033\n","Iteration 111, loss = 0.09984762\n","Iteration 112, loss = 0.09921858\n","Iteration 113, loss = 0.09539198\n","Iteration 114, loss = 0.09490815\n","Iteration 115, loss = 0.10138111\n","Iteration 116, loss = 0.09494531\n","Iteration 117, loss = 0.09297968\n","Iteration 118, loss = 0.09230570\n","Iteration 119, loss = 0.09178463\n","Iteration 120, loss = 0.09141526\n","Iteration 121, loss = 0.09004303\n","Iteration 122, loss = 0.09075909\n","Iteration 123, loss = 0.08705671\n","Iteration 124, loss = 0.08473954\n","Iteration 125, loss = 0.08616853\n","Iteration 126, loss = 0.08380266\n","Iteration 127, loss = 0.08509186\n","Iteration 128, loss = 0.08227856\n","Iteration 129, loss = 0.08358145\n","Iteration 130, loss = 0.08660971\n","Iteration 131, loss = 0.08283450\n","Iteration 132, loss = 0.08478419\n","Iteration 133, loss = 0.08178700\n","Iteration 134, loss = 0.08209876\n","Iteration 135, loss = 0.07849708\n","Iteration 136, loss = 0.08000138\n","Iteration 137, loss = 0.07937349\n","Iteration 138, loss = 0.07580139\n","Iteration 139, loss = 0.07587425\n","Iteration 140, loss = 0.07868398\n","Iteration 141, loss = 0.07661592\n","Iteration 142, loss = 0.07545286\n","Iteration 143, loss = 0.07723285\n","Iteration 144, loss = 0.07805638\n","Iteration 145, loss = 0.07436343\n","Iteration 146, loss = 0.07215661\n","Iteration 147, loss = 0.07164641\n","Iteration 148, loss = 0.07107009\n","Iteration 149, loss = 0.06841899\n","Iteration 150, loss = 0.06804047\n","Iteration 151, loss = 0.07186777\n","Iteration 152, loss = 0.06739316\n","Iteration 153, loss = 0.06976913\n","Iteration 154, loss = 0.06822091\n","Iteration 155, loss = 0.06684248\n","Iteration 156, loss = 0.06745573\n","Iteration 157, loss = 0.06590818\n","Iteration 158, loss = 0.06882316\n","Iteration 159, loss = 0.06643517\n","Iteration 160, loss = 0.06764438\n","Iteration 161, loss = 0.06512082\n","Iteration 162, loss = 0.06507455\n","Iteration 163, loss = 0.06561580\n","Iteration 164, loss = 0.06270159\n","Iteration 165, loss = 0.06539471\n","Iteration 166, loss = 0.06475039\n","Iteration 167, loss = 0.06624432\n","Iteration 168, loss = 0.06263186\n","Iteration 169, loss = 0.06477232\n","Iteration 170, loss = 0.06143500\n","Iteration 171, loss = 0.06295565\n","Iteration 172, loss = 0.06153751\n","Iteration 173, loss = 0.06370259\n","Iteration 174, loss = 0.06119413\n","Iteration 175, loss = 0.06670475\n","Iteration 176, loss = 0.06333047\n","Iteration 177, loss = 0.06312487\n","Iteration 178, loss = 0.06047734\n","Iteration 179, loss = 0.05964190\n","Iteration 180, loss = 0.05898970\n","Iteration 181, loss = 0.05723685\n","Iteration 182, loss = 0.05698896\n","Iteration 183, loss = 0.05647954\n","Iteration 184, loss = 0.05545568\n","Iteration 185, loss = 0.05466866\n","Iteration 186, loss = 0.05376985\n","Iteration 187, loss = 0.05355011\n","Iteration 188, loss = 0.05388575\n","Iteration 189, loss = 0.05494336\n","Iteration 190, loss = 0.05530063\n","Iteration 191, loss = 0.05395298\n","Iteration 192, loss = 0.05778237\n","Iteration 193, loss = 0.06889828\n","Iteration 194, loss = 0.06775794\n","Iteration 195, loss = 0.06322826\n","Iteration 196, loss = 0.06352294\n","Iteration 197, loss = 0.05899847\n","Iteration 198, loss = 0.05690716\n","Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"]},{"data":{"text/plain":["MLPClassifier(hidden_layer_sizes=(8, 8, 8), learning_rate_init=0.01,\n","              max_iter=3000, tol=1e-07, verbose=True)"]},"execution_count":475,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"5F0QbRa2q9jH"},"source":["### Treinamento: Um modelo de Regressão Logística para comparação"],"id":"5F0QbRa2q9jH"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPs61O_jrE1-","executionInfo":{"elapsed":218,"status":"ok","timestamp":1637353870134,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"211e0e6c-4607-4288-9f99-e62996db9185"},"source":["rl = LogisticRegression(max_iter=2000,\n","                        tol=0.0001,\n","                        )\n","rl.fit(xTrain, yTrain)"],"id":"yPs61O_jrE1-","execution_count":null,"outputs":[{"data":{"text/plain":["LogisticRegression(max_iter=2000)"]},"execution_count":476,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"XT9VreKXrM12"},"source":["### Teste: Avaliando a precisão dos modelos"],"id":"XT9VreKXrM12"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2b554b5","executionInfo":{"elapsed":3,"status":"ok","timestamp":1637353872218,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"9c39a19a-f22a-4f28-ee34-e68a3318f8fd"},"source":["yPredictedRna = rna.predict(xTest)\n","yPredictedRl = rl.predict(xTest)\n","\n","print(\"Acuracia RNA: \",accuracy_score(yTest,yPredictedRna)) #rna.score(xTestNorm, yTest)\n","print(\"Acuracia Logistic Regression: \",accuracy_score(yTest,yPredictedRl))"],"id":"b2b554b5","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Acuracia RNA:  0.8506787330316742\n","Acuracia Logistic Regression:  0.8914027149321267\n"]}]},{"cell_type":"markdown","metadata":{"id":"782d5503"},"source":["### Métricas de F1"],"id":"782d5503"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3npTfhct42Lk","executionInfo":{"elapsed":199,"status":"ok","timestamp":1637353873478,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"ba5a5235-18e3-40d2-fa77-6f8f8886c6e5"},"source":["print(classification_report(yTest, yPredictedRna, labels=[0, 1]))"],"id":"3npTfhct42Lk","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.82      0.83        96\n","           1       0.87      0.87      0.87       125\n","\n","    accuracy                           0.85       221\n","   macro avg       0.85      0.85      0.85       221\n","weighted avg       0.85      0.85      0.85       221\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmugoZM05Id0","executionInfo":{"elapsed":194,"status":"ok","timestamp":1637353874837,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"ceb889a1-2539-469e-9862-72a7594f9dee"},"source":["print(classification_report(yTest, yPredictedRl, labels=[0, 1]))"],"id":"CmugoZM05Id0","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.90      0.84      0.87        96\n","           1       0.89      0.93      0.91       125\n","\n","    accuracy                           0.89       221\n","   macro avg       0.89      0.89      0.89       221\n","weighted avg       0.89      0.89      0.89       221\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"c34b1b0f"},"source":["### Matriz de confusão"],"id":"c34b1b0f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"9a64f9e1","executionInfo":{"elapsed":554,"status":"ok","timestamp":1637353876599,"user":{"displayName":"Frank Brito","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj-gpjmkKMfsOS4FInrL_qoVqDP9Adxj_NaVcCwxOM=s64","userId":"08307313002022178292"},"user_tz":180},"outputId":"6fcc0fb4-c6c7-44ed-8310-041f031e422b"},"source":["cm = confusion_matrix(yTest, yPredictedRna)\n","cmd_obj = ConfusionMatrixDisplay(cm, display_labels=['Sim', 'Não'])\n","cmd_obj.plot()\n","\n","cmd_obj.ax_.set(\n","                title='Matriz de confusão RNA', \n","                xlabel='Predito', \n","                ylabel='Real')\n","plt.show()\n"],"id":"9a64f9e1","execution_count":null,"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"id":"055d1b76","outputId":"48dfdf74-3063-48ba-f2a3-bfce21462c34"},"source":["cm = confusion_matrix(yTest, yPredictedRl)\n","cmd_obj = ConfusionMatrixDisplay(cm, display_labels=['Sim', 'Não'])\n","cmd_obj.plot()\n","\n","cmd_obj.ax_.set(\n","                title='Matriz de confusão RL', \n","                xlabel='Predito', \n","                ylabel='Real')\n","plt.show()"],"id":"055d1b76","execution_count":null,"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"markdown","metadata":{"id":"dd44f3b7"},"source":["### Resultado"],"id":"dd44f3b7"},{"cell_type":"markdown","metadata":{"id":"9d476d80"},"source":["A partir dos experimentos feitos para este dataset, foi possível observar que a regressão logística apresentou uma acurácia superior ao modelo de rede neural. Foram feitos diversos treinamentos com modificações na parametrização dos modelos e em sua maioria a RNA se mostrou com acurácia inferior à regressão logística. Porém, ambos os modelos apresentaram resultados próximos, com uma boa taxa de acerto nas predições."],"id":"9d476d80"}]}